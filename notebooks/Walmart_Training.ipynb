{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e12b06-d6be-4128-8e44-36d76e285941",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-07-10T06:53:27.921280Z",
     "iopub.status.busy": "2025-07-10T06:53:27.920917Z",
     "iopub.status.idle": "2025-07-10T06:53:34.889982Z",
     "shell.execute_reply": "2025-07-10T06:53:34.889016Z",
     "shell.execute_reply.started": "2025-07-10T06:53:27.921249Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: odps in /opt/conda/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: pyodps in /opt/conda/lib/python3.11/site-packages (from odps) (0.12.3)\n",
      "Requirement already satisfied: requests>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from pyodps->odps) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from pyodps->odps) (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.11/site-packages (from pyarrow>=2.0.0->pyodps->odps) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.4.0->pyodps->odps) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.4.0->pyodps->odps) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.4.0->pyodps->odps) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.4.0->pyodps->odps) (2025.1.31)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (1.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (6.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install odps\n",
    "#!pip install scikit-learn\n",
    "#!pip install joblib\n",
    "#!pip install pyyaml\n",
    "#!pip install \"alipai>=0.4.0\"\n",
    "\n",
    "\"\"\"\n",
    "try:\n",
    "    import pai\n",
    "    print(\"âœ… PAI SDKå®‰è£…æˆåŠŸï¼\")\n",
    "    print(f\"ç‰ˆæœ¬: {pai.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(\"âŒ PAI SDKæœªå®‰è£…\")\n",
    "    print(f\"é”™è¯¯: {e}\")\n",
    "\n",
    "# æ£€æŸ¥æ‰€æœ‰å·²å®‰è£…çš„PAIç›¸å…³åŒ…\n",
    "import subprocess\n",
    "result = subprocess.run(['pip', 'list'], capture_output=True, text=True)\n",
    "pai_packages = [line for line in result.stdout.split('\\n') if 'pai' in line.lower()]\n",
    "if pai_packages:\n",
    "    print(\"\\nå·²å®‰è£…çš„PAIç›¸å…³åŒ…:\")\n",
    "    for pkg in pai_packages:\n",
    "        print(f\"  {pkg}\")\n",
    "else:\n",
    "    print(\"\\næœªæ‰¾åˆ°PAIç›¸å…³åŒ…\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad728b5-b9f0-4f94-91b5-7f442cb5aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walmart_Training.ipynb - å®‰å…¨ç‰ˆæœ¬ï¼ˆç§»é™¤æ‰€æœ‰æ•æ„Ÿä¿¡æ¯ï¼‰\n",
    "# å¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒç¬”è®°æœ¬ - é›†æˆGitç‰ˆæœ¬ç®¡ç†\n",
    "\n",
    "# Part 1: ç¯å¢ƒè®¾ç½®å’Œç‰ˆæœ¬è¿½è¸ª\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import joblib\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "print(\"=== Walmarté”€é‡é¢„æµ‹æ¨¡å‹è®­ç»ƒ - ç‰ˆæœ¬å¯è¿½è¸ª ===\")\n",
    "\n",
    "# Part 2: Gitç‰ˆæœ¬ä¿¡æ¯è·å–\n",
    "def get_git_version_info():\n",
    "    \"\"\"è·å–Gitç‰ˆæœ¬ä¿¡æ¯ç”¨äºæ¨¡å‹è¿½æº¯\"\"\"\n",
    "    try:\n",
    "        # è·å–å½“å‰commit ID\n",
    "        commit_id = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()\n",
    "        \n",
    "        # è·å–å½“å‰åˆ†æ”¯\n",
    "        branch = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD']).decode('utf-8').strip()\n",
    "        \n",
    "        # è·å–æœ€åæäº¤ä¿¡æ¯\n",
    "        commit_message = subprocess.check_output(['git', 'log', '-1', '--pretty=%B']).decode('utf-8').strip()\n",
    "        \n",
    "        # è·å–æäº¤æ—¶é—´\n",
    "        commit_time = subprocess.check_output(['git', 'log', '-1', '--pretty=%ci']).decode('utf-8').strip()\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦æœ‰æœªæäº¤çš„ä¿®æ”¹\n",
    "        status = subprocess.check_output(['git', 'status', '--porcelain']).decode('utf-8').strip()\n",
    "        has_uncommitted = len(status) > 0\n",
    "        \n",
    "        version_info = {\n",
    "            'git_commit_id': commit_id,\n",
    "            'git_branch': branch,\n",
    "            'commit_message': commit_message,\n",
    "            'commit_time': commit_time,\n",
    "            'has_uncommitted_changes': has_uncommitted,\n",
    "            'repository_url': 'https://github.com/ä½ çš„ç”¨æˆ·å/walmart-pai-demo',  # è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…ç”¨æˆ·å\n",
    "            'training_script': 'notebooks/Walmart_Training.ipynb'\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Gitç‰ˆæœ¬ä¿¡æ¯:\")\n",
    "        print(f\"   Commit ID: {commit_id[:8]}...\")\n",
    "        print(f\"   åˆ†æ”¯: {branch}\")\n",
    "        print(f\"   å¯å¤ç°: {'å¦ï¼ˆæœ‰æœªæäº¤ä¿®æ”¹ï¼‰' if has_uncommitted else 'æ˜¯'}\")\n",
    "        \n",
    "        return version_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è·å–Gitä¿¡æ¯å¤±è´¥: {e}\")\n",
    "        return {\n",
    "            'git_commit_id': 'unknown',\n",
    "            'git_branch': 'unknown',\n",
    "            'error': str(e),\n",
    "            'repository_url': 'https://github.com/ä½ çš„ç”¨æˆ·å/walmart-pai-demo'  # è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…ç”¨æˆ·å\n",
    "        }\n",
    "\n",
    "# è·å–ç‰ˆæœ¬ä¿¡æ¯\n",
    "git_info = get_git_version_info()\n",
    "\n",
    "# Part 3: å®‰å…¨çš„ç¯å¢ƒé…ç½®åŠ è½½\n",
    "def load_config_safely():\n",
    "    \"\"\"å®‰å…¨åœ°åŠ è½½é…ç½®æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç§æœ‰é…ç½®\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”§ åŠ è½½é…ç½®æ–‡ä»¶...\")\n",
    "    \n",
    "    try:\n",
    "        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç§æœ‰é…ç½®æ–‡ä»¶ï¼ˆåŒ…å«çœŸå®å¯†é’¥ï¼‰\n",
    "        if os.path.exists('config_local.yaml'):\n",
    "            with open('config_local.yaml', 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            print(\"âœ… ä½¿ç”¨æœ¬åœ°ç§æœ‰é…ç½®æ–‡ä»¶ (config_local.yaml)\")\n",
    "            return config\n",
    "        \n",
    "        # å¦‚æœæ²¡æœ‰æœ¬åœ°é…ç½®ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–\n",
    "        elif all([os.getenv('ODPS_ACCESS_ID'), os.getenv('ODPS_ACCESS_KEY')]):\n",
    "            print(\"âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®\")\n",
    "            config = {\n",
    "                'maxcompute': {\n",
    "                    'access_id': os.getenv('ODPS_ACCESS_ID'),\n",
    "                    'access_key': os.getenv('ODPS_ACCESS_KEY'),\n",
    "                    'project': os.getenv('ODPS_PROJECT', 'ds_case_demo'),\n",
    "                    'endpoint': os.getenv('ODPS_ENDPOINT', 'https://service.cn-shanghai.maxcompute.aliyun.com/api')\n",
    "                }\n",
    "            }\n",
    "            return config\n",
    "        \n",
    "        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨æ¨¡æ¿é…ç½®ï¼ˆä½†ç»™å‡ºè­¦å‘Šï¼‰\n",
    "        elif os.path.exists('config.yaml'):\n",
    "            with open('config.yaml', 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            print(\"âš ï¸ ä½¿ç”¨æ¨¡æ¿é…ç½®æ–‡ä»¶ï¼Œè¯·ç¡®ä¿å·²é…ç½®çœŸå®å¯†é’¥\")\n",
    "            print(\"å»ºè®®ï¼šåˆ›å»º config_local.yaml æ–‡ä»¶æˆ–è®¾ç½®ç¯å¢ƒå˜é‡\")\n",
    "            return config\n",
    "        \n",
    "        else:\n",
    "            raise FileNotFoundError(\"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}\")\n",
    "        raise\n",
    "\n",
    "def setup_odps_connection(config):\n",
    "    \"\"\"å®‰å…¨åœ°è®¾ç½®MaxComputeè¿æ¥\"\"\"\n",
    "    \n",
    "    try:\n",
    "        maxcompute_config = config.get('maxcompute', {})\n",
    "        \n",
    "        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä¸æ‰“å°æ•æ„Ÿä¿¡æ¯ï¼‰\n",
    "        os.environ['ODPS_ACCESS_ID'] = maxcompute_config.get('access_id', '')\n",
    "        os.environ['ODPS_ACCESS_KEY'] = maxcompute_config.get('access_key', '')\n",
    "        os.environ['ODPS_PROJECT'] = maxcompute_config.get('project', 'ds_case_demo')\n",
    "        os.environ['ODPS_ENDPOINT'] = maxcompute_config.get('endpoint', 'https://service.cn-shanghai.maxcompute.aliyun.com/api')\n",
    "        \n",
    "        # éªŒè¯é…ç½®ï¼ˆä¸æ˜¾ç¤ºæ•æ„Ÿä¿¡æ¯ï¼‰\n",
    "        if not os.environ['ODPS_ACCESS_ID'] or 'AccessKey' in os.environ['ODPS_ACCESS_ID']:\n",
    "            print(\"âš ï¸ æ£€æµ‹åˆ°å ä½ç¬¦AccessKeyï¼Œè¯·ç¡®ä¿ä½¿ç”¨çœŸå®é…ç½®\")\n",
    "        else:\n",
    "            print(f\"âœ… MaxComputeè¿æ¥é…ç½®å®Œæˆ\")\n",
    "            print(f\"   é¡¹ç›®: {os.environ['ODPS_PROJECT']}\")\n",
    "            print(f\"   åœ°åŸŸ: {os.environ['ODPS_ENDPOINT'].split('.')[-4] if '.' in os.environ['ODPS_ENDPOINT'] else 'unknown'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è®¾ç½®MaxComputeè¿æ¥å¤±è´¥: {e}\")\n",
    "        raise\n",
    "\n",
    "# åŠ è½½é…ç½®å¹¶è®¾ç½®è¿æ¥\n",
    "config = load_config_safely()\n",
    "setup_odps_connection(config)\n",
    "\n",
    "# Part 4: é…ç½®ç±»\n",
    "class TrainingConfig:\n",
    "    def __init__(self, config_dict: Dict[str, Any] = None):\n",
    "        if config_dict:\n",
    "            self.config = self._merge_with_defaults(config_dict)\n",
    "        else:\n",
    "            self.config = self._get_default_config()\n",
    "    \n",
    "    def _get_default_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–é»˜è®¤é…ç½®\"\"\"\n",
    "        return {\n",
    "            \"data_source\": {\n",
    "                \"train_table\": \"walmart_train_vif\",\n",
    "                \"target_column\": \"weekly_sales\"\n",
    "            },\n",
    "            \"training\": {\n",
    "                \"validation_split\": 0.2,\n",
    "                \"random_state\": 42\n",
    "            },\n",
    "            \"models\": {\n",
    "                \"linear_regression\": {\"enabled\": True, \"params\": {}},\n",
    "                \"elastic_net\": {\n",
    "                    \"enabled\": True,\n",
    "                    \"param_grid\": {\n",
    "                        \"alpha\": [0.1, 0.5, 1.0, 2.0],\n",
    "                        \"l1_ratio\": [0.1, 0.5, 0.9]\n",
    "                    }\n",
    "                },\n",
    "                \"random_forest\": {\n",
    "                    \"enabled\": True,\n",
    "                    \"param_grid\": {\n",
    "                        \"n_estimators\": [50, 100],\n",
    "                        \"max_depth\": [10, 20],\n",
    "                        \"min_samples_split\": [2, 5]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"model_dir\": \"/mnt/workspace/models\",\n",
    "                \"log_level\": \"INFO\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _merge_with_defaults(self, config_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"å°†ç”¨æˆ·é…ç½®ä¸é»˜è®¤é…ç½®åˆå¹¶\"\"\"\n",
    "        default_config = self._get_default_config()\n",
    "        \n",
    "        # ç®€å•çš„å­—å…¸åˆå¹¶ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦å®ç°æ›´å¤æ‚çš„åˆå¹¶é€»è¾‘ï¼‰\n",
    "        if 'data' in config_dict:\n",
    "            if 'train_table' in config_dict['data']:\n",
    "                default_config['data_source']['train_table'] = config_dict['data']['train_table']\n",
    "            if 'target_column' in config_dict['data']:\n",
    "                default_config['data_source']['target_column'] = config_dict['data']['target_column']\n",
    "        \n",
    "        if 'training' in config_dict:\n",
    "            default_config['training'].update(config_dict['training'])\n",
    "        \n",
    "        if 'output' in config_dict:\n",
    "            default_config['output'].update(config_dict['output'])\n",
    "        \n",
    "        return default_config\n",
    "\n",
    "# Part 5: å¢å¼ºç‰ˆæ¨¡å‹åŒ…è£…å™¨\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model, model_name: str, feature_columns: list, metrics: Dict[str, float], git_info: dict = None):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.feature_columns = feature_columns\n",
    "        self.metrics = metrics\n",
    "        self.git_info = git_info or {}\n",
    "        self.created_at = datetime.now()\n",
    "        self.model_version = f\"v_{self.created_at.strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"æ ‡å‡†åŒ–é¢„æµ‹æ¥å£\"\"\"\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            return self.model.predict(data[self.feature_columns])\n",
    "        else:\n",
    "            return self.model.predict(data)\n",
    "    \n",
    "    def get_model_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–åŒ…å«Gitç‰ˆæœ¬ä¿¡æ¯çš„å®Œæ•´æ¨¡å‹ä¿¡æ¯\"\"\"\n",
    "        base_info = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"model_type\": type(self.model).__name__,\n",
    "            \"model_version\": self.model_version,\n",
    "            \"feature_count\": len(self.feature_columns),\n",
    "            \"features\": self.feature_columns,\n",
    "            \"metrics\": self.metrics,\n",
    "            \"created_at\": self.created_at.isoformat(),\n",
    "            \"framework\": \"sklearn\"\n",
    "        }\n",
    "        \n",
    "        # æ·»åŠ Gitç‰ˆæœ¬ä¿¡æ¯ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰\n",
    "        base_info.update({\n",
    "            'code_version': {\n",
    "                'git_commit_id': self.git_info.get('git_commit_id'),\n",
    "                'git_branch': self.git_info.get('git_branch'),\n",
    "                'commit_message': self.git_info.get('commit_message'),\n",
    "                'commit_time': self.git_info.get('commit_time'),\n",
    "                'repository_url': self.git_info.get('repository_url'),\n",
    "                'training_script': self.git_info.get('training_script')\n",
    "            },\n",
    "            'reproducibility': {\n",
    "                'can_reproduce': not self.git_info.get('has_uncommitted_changes', True),\n",
    "                'reproduction_command': f\"git checkout {self.git_info.get('git_commit_id', 'unknown')}\",\n",
    "                'reproduction_steps': [\n",
    "                    f\"git clone {self.git_info.get('repository_url', 'your-repo')}\",\n",
    "                    f\"git checkout {self.git_info.get('git_commit_id', 'unknown')}\",\n",
    "                    \"pip install -r requirements.txt\",\n",
    "                    \"jupyter notebook notebooks/Walmart_Training.ipynb\"\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        return base_info\n",
    "    \n",
    "    def save(self, base_dir: str) -> str:\n",
    "        \"\"\"ä¿å­˜æ¨¡å‹å’Œå®Œæ•´å…ƒæ•°æ®\"\"\"\n",
    "        model_dir = os.path.join(base_dir, f\"{self.model_name}_{self.model_version}\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        model_path = os.path.join(model_dir, \"model.pkl\")\n",
    "        joblib.dump(self.model, model_path)\n",
    "        \n",
    "        # ä¿å­˜å®Œæ•´å…ƒæ•°æ®ï¼ˆåŒ…å«ç‰ˆæœ¬ä¿¡æ¯ï¼‰\n",
    "        metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(self.get_model_info(), f, indent=2)\n",
    "        \n",
    "        # ä¿å­˜å¯å¤ç°è„šæœ¬\n",
    "        if not self.git_info.get('has_uncommitted_changes', True):\n",
    "            reproduce_script = f\"\"\"#!/bin/bash\n",
    "# æ¨¡å‹å¤ç°è„šæœ¬ - è‡ªåŠ¨ç”Ÿæˆ\n",
    "# æ¨¡å‹: {self.model_name}\n",
    "# è®­ç»ƒæ—¶é—´: {self.created_at}\n",
    "\n",
    "echo \"å¼€å§‹å¤ç°æ¨¡å‹è®­ç»ƒ...\"\n",
    "git clone {self.git_info.get('repository_url', 'your-repo')}\n",
    "cd walmart-pai-demo\n",
    "git checkout {self.git_info.get('git_commit_id', 'unknown')}\n",
    "pip install -r requirements.txt\n",
    "echo \"ç¯å¢ƒå‡†å¤‡å®Œæˆï¼Œè¯·è¿è¡Œ: jupyter notebook notebooks/Walmart_Training.ipynb\"\n",
    "\"\"\"\n",
    "            reproduce_path = os.path.join(model_dir, \"reproduce.sh\")\n",
    "            with open(reproduce_path, 'w') as f:\n",
    "                f.write(reproduce_script)\n",
    "        \n",
    "        print(f\"âœ… æ¨¡å‹å·²ä¿å­˜: {model_dir}\")\n",
    "        return model_dir\n",
    "\n",
    "# Part 6: è®­ç»ƒç®¡ç†å™¨\n",
    "class WalmartTrainingManager:\n",
    "    def __init__(self, config: TrainingConfig, git_info: dict):\n",
    "        self.config = config\n",
    "        self.git_info = git_info\n",
    "        self.logger = self._setup_logger()\n",
    "        self.training_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # è®­ç»ƒå…ƒæ•°æ®ï¼ˆåŒ…å«Gitç‰ˆæœ¬ä¿¡æ¯ï¼Œä½†ä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰\n",
    "        self.training_metadata = {\n",
    "            \"training_id\": self.training_id,\n",
    "            \"start_time\": datetime.now().isoformat(),\n",
    "            \"config\": self.config.config,\n",
    "            \"code_version\": git_info,  # æ ¸å¿ƒï¼šè®°å½•ä»£ç ç‰ˆæœ¬\n",
    "            \"reproducibility\": {\n",
    "                \"can_reproduce\": not git_info.get('has_uncommitted_changes', True),\n",
    "                \"reproduction_steps\": [\n",
    "                    f\"git clone {git_info.get('repository_url', 'your-repo')}\",\n",
    "                    f\"git checkout {git_info.get('git_commit_id', 'unknown')}\",\n",
    "                    \"pip install -r requirements.txt\",\n",
    "                    \"åˆ›å»º config_local.yaml å¹¶é…ç½®çœŸå®å¯†é’¥\",\n",
    "                    \"jupyter notebook notebooks/Walmart_Training.ipynb\"\n",
    "                ]\n",
    "            },\n",
    "            \"models\": {},\n",
    "            \"status\": \"running\"\n",
    "        }\n",
    "    \n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        \"\"\"è®¾ç½®æ—¥å¿—\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, self.config.config[\"output\"][\"log_level\"]),\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        return logging.getLogger(__name__)\n",
    "    \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"åŠ è½½è®­ç»ƒæ•°æ®\"\"\"\n",
    "        self.logger.info(f\"[{self.training_id}] å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®\")\n",
    "        \n",
    "        try:\n",
    "            # ä»MaxComputeåŠ è½½\n",
    "            train_df = self._load_from_maxcompute()\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"ä»MaxComputeåŠ è½½å¤±è´¥: {e}\")\n",
    "            # å¤‡é€‰æ–¹æ¡ˆï¼šä»æœ¬åœ°æ–‡ä»¶åŠ è½½\n",
    "            train_df = self._load_from_local()\n",
    "        \n",
    "        self.logger.info(f\"åŸå§‹è®­ç»ƒé›†å½¢çŠ¶: {train_df.shape}\")\n",
    "        \n",
    "        # è®°å½•æ•°æ®ç‰ˆæœ¬ä¿¡æ¯ï¼ˆä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰\n",
    "        self.training_metadata[\"data_info\"] = {\n",
    "            \"original_train_shape\": train_df.shape,\n",
    "            \"train_table\": self.config.config[\"data_source\"][\"train_table\"],\n",
    "            \"validation_split\": self.config.config[\"training\"][\"validation_split\"]\n",
    "        }\n",
    "        \n",
    "        return train_df\n",
    "    \n",
    "    def _load_from_maxcompute(self) -> pd.DataFrame:\n",
    "        \"\"\"ä»MaxComputeåŠ è½½è®­ç»ƒæ•°æ®\"\"\"\n",
    "        from odps import ODPS\n",
    "        \n",
    "        # ä»ç¯å¢ƒå˜é‡è·å–è¿æ¥ä¿¡æ¯ï¼ˆå·²åœ¨å‰é¢å®‰å…¨è®¾ç½®ï¼‰\n",
    "        access_id = os.getenv('ODPS_ACCESS_ID')\n",
    "        access_key = os.getenv('ODPS_ACCESS_KEY')\n",
    "        project = os.getenv('ODPS_PROJECT')\n",
    "        endpoint = os.getenv('ODPS_ENDPOINT')\n",
    "        \n",
    "        if not all([access_id, access_key, project, endpoint]):\n",
    "            raise ValueError(\"MaxComputeè¿æ¥ä¿¡æ¯ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥é…ç½®\")\n",
    "        \n",
    "        odps = ODPS(access_id, access_key, project, endpoint)\n",
    "        train_table = odps.get_table(self.config.config[\"data_source\"][\"train_table\"])\n",
    "        train_df = train_table.to_df().to_pandas()\n",
    "        \n",
    "        return train_df\n",
    "    \n",
    "    def _load_from_local(self) -> pd.DataFrame:\n",
    "        \"\"\"ä»æœ¬åœ°æ–‡ä»¶åŠ è½½è®­ç»ƒæ•°æ®\"\"\"\n",
    "        train_path = f'/mnt/workspace/data/{self.config.config[\"data_source\"][\"train_table\"]}.csv'\n",
    "        if not os.path.exists(train_path):\n",
    "            # å°è¯•ç›¸å¯¹è·¯å¾„\n",
    "            train_path = f'data/walmart_train_data.csv'\n",
    "        \n",
    "        if os.path.exists(train_path):\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            return train_df\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶: {train_path}\")\n",
    "    \n",
    "    def prepare_features(self, train_df: pd.DataFrame) -> Tuple:\n",
    "        \"\"\"å‡†å¤‡ç‰¹å¾æ•°æ®å¹¶è¿›è¡Œè®­ç»ƒéªŒè¯æ‹†åˆ†\"\"\"\n",
    "        target_column = self.config.config[\"data_source\"][\"target_column\"]\n",
    "        feature_columns = [col for col in train_df.columns if col != target_column]\n",
    "        \n",
    "        X = train_df[feature_columns]\n",
    "        y = train_df[target_column]\n",
    "        \n",
    "        # è¿›è¡Œè®­ç»ƒéªŒè¯æ‹†åˆ†\n",
    "        validation_split = self.config.config[\"training\"][\"validation_split\"]\n",
    "        random_state = self.config.config[\"training\"][\"random_state\"]\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=validation_split, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"ç‰¹å¾æ•°é‡: {len(feature_columns)}\")\n",
    "        self.logger.info(f\"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train)}, éªŒè¯é›†æ ·æœ¬æ•°: {len(X_val)}\")\n",
    "        \n",
    "        return X_train, y_train, X_val, y_val, feature_columns\n",
    "    \n",
    "    def train_model(self, model_name: str, X_train, y_train, X_val, y_val) -> Optional[ModelWrapper]:\n",
    "        \"\"\"è®­ç»ƒå•ä¸ªæ¨¡å‹ï¼ˆé›†æˆç‰ˆæœ¬è¿½è¸ªï¼‰\"\"\"\n",
    "        if not self.config.config[\"models\"][model_name][\"enabled\"]:\n",
    "            self.logger.info(f\"æ¨¡å‹ {model_name} å·²ç¦ç”¨ï¼Œè·³è¿‡è®­ç»ƒ\")\n",
    "            return None\n",
    "        \n",
    "        self.logger.info(f\"å¼€å§‹è®­ç»ƒæ¨¡å‹: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            if model_name == \"linear_regression\":\n",
    "                model, metrics = self._train_linear_regression(X_train, y_train, X_val, y_val)\n",
    "            elif model_name == \"elastic_net\":\n",
    "                model, metrics = self._train_elastic_net(X_train, y_train, X_val, y_val)\n",
    "            elif model_name == \"random_forest\":\n",
    "                model, metrics = self._train_random_forest(X_train, y_train, X_val, y_val)\n",
    "            else:\n",
    "                raise ValueError(f\"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_name}\")\n",
    "            \n",
    "            # ä½¿ç”¨å¢å¼ºç‰ˆModelWrapperï¼ˆåŒ…å«Gitä¿¡æ¯ï¼‰\n",
    "            wrapped_model = ModelWrapper(\n",
    "                model=model, \n",
    "                model_name=model_name, \n",
    "                feature_columns=X_train.columns.tolist(), \n",
    "                metrics=metrics,\n",
    "                git_info=self.git_info  # ä¼ å…¥Gitç‰ˆæœ¬ä¿¡æ¯\n",
    "            )\n",
    "            \n",
    "            # è®°å½•è®­ç»ƒç»“æœ\n",
    "            self.training_metadata[\"models\"][model_name] = {\n",
    "                \"status\": \"success\",\n",
    "                \"metrics\": metrics,\n",
    "                \"model_version\": wrapped_model.model_version,\n",
    "                \"git_commit_id\": self.git_info.get('git_commit_id')  # è®°å½•ä»£ç ç‰ˆæœ¬\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"æ¨¡å‹ {model_name} è®­ç»ƒå®Œæˆï¼ŒéªŒè¯é›†RÂ²: {metrics.get('val_r2', 'N/A'):.4f}\")\n",
    "            return wrapped_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"æ¨¡å‹ {model_name} è®­ç»ƒå¤±è´¥: {e}\")\n",
    "            self.training_metadata[\"models\"][model_name] = {\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            return None\n",
    "    \n",
    "    def _train_linear_regression(self, X_train, y_train, X_val, y_val) -> Tuple:\n",
    "        \"\"\"è®­ç»ƒçº¿æ€§å›å½’\"\"\"\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = self._calculate_metrics(model, X_train, y_train, X_val, y_val)\n",
    "        return model, metrics\n",
    "    \n",
    "    def _train_elastic_net(self, X_train, y_train, X_val, y_val) -> Tuple:\n",
    "        \"\"\"è®­ç»ƒå¼¹æ€§ç½‘ç»œ\"\"\"\n",
    "        param_grid = self.config.config[\"models\"][\"elastic_net\"][\"param_grid\"]\n",
    "        elastic_net = ElasticNet(random_state=42, max_iter=1000)\n",
    "        grid_search = GridSearchCV(elastic_net, param_grid, cv=3, scoring='r2', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        metrics = self._calculate_metrics(best_model, X_train, y_train, X_val, y_val)\n",
    "        metrics['best_params'] = grid_search.best_params_\n",
    "        \n",
    "        return best_model, metrics\n",
    "    \n",
    "    def _train_random_forest(self, X_train, y_train, X_val, y_val) -> Tuple:\n",
    "        \"\"\"è®­ç»ƒéšæœºæ£®æ—\"\"\"\n",
    "        param_grid = self.config.config[\"models\"][\"random_forest\"][\"param_grid\"]\n",
    "        rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='r2', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        metrics = self._calculate_metrics(best_model, X_train, y_train, X_val, y_val)\n",
    "        metrics['best_params'] = grid_search.best_params_\n",
    "        metrics['feature_importance'] = best_model.feature_importances_.tolist()\n",
    "        \n",
    "        return best_model, metrics\n",
    "    \n",
    "    def _calculate_metrics(self, model, X_train, y_train, X_val, y_val) -> Dict[str, float]:\n",
    "        \"\"\"è®¡ç®—æ¨¡å‹æ€§èƒ½æŒ‡æ ‡\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # è®­ç»ƒé›†æŒ‡æ ‡\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        metrics['train_r2'] = r2_score(y_train, y_pred_train)\n",
    "        metrics['train_mse'] = mean_squared_error(y_train, y_pred_train)\n",
    "        metrics['train_mae'] = mean_absolute_error(y_train, y_pred_train)\n",
    "        \n",
    "        # éªŒè¯é›†æŒ‡æ ‡\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        metrics['val_r2'] = r2_score(y_val, y_pred_val)\n",
    "        metrics['val_mse'] = mean_squared_error(y_val, y_pred_val)\n",
    "        metrics['val_mae'] = mean_absolute_error(y_val, y_pred_val)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def save_training_summary(self, model_paths: Dict[str, str]):\n",
    "        \"\"\"ä¿å­˜è®­ç»ƒæ€»ç»“ï¼ˆåŒ…å«å®Œæ•´ç‰ˆæœ¬ä¿¡æ¯ï¼Œä½†ä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰\"\"\"\n",
    "        self.training_metadata[\"end_time\"] = datetime.now().isoformat()\n",
    "        self.training_metadata[\"status\"] = \"completed\"\n",
    "        self.training_metadata[\"model_paths\"] = model_paths\n",
    "        \n",
    "        # ä¿å­˜æ€»ç»“æ–‡ä»¶\n",
    "        summary_path = os.path.join(\n",
    "            self.config.config[\"output\"][\"model_dir\"],\n",
    "            f\"training_summary_{self.training_id}.json\"\n",
    "        )\n",
    "        \n",
    "        # ç¡®ä¿ç›®å½•å­˜åœ¨\n",
    "        os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "        \n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(self.training_metadata, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"è®­ç»ƒæ€»ç»“å·²ä¿å­˜: {summary_path}\")\n",
    "    \n",
    "    def register_models(self, models: Dict[str, ModelWrapper]) -> Dict[str, bool]:\n",
    "        \"\"\"æ³¨å†Œæ¨¡å‹åˆ°PAI Model Registryï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰\"\"\"\n",
    "        self.logger.info(\"å¼€å§‹æ³¨å†Œæ¨¡å‹åˆ°PAI Model Registry...\")\n",
    "        \n",
    "        registration_results = {}\n",
    "        \n",
    "        for model_name, model_wrapper in models.items():\n",
    "            if model_wrapper is not None:\n",
    "                try:\n",
    "                    success = self._register_single_model(model_wrapper)\n",
    "                    registration_results[model_name] = success\n",
    "                    if success:\n",
    "                        self.logger.info(f\"âœ… æ¨¡å‹ {model_name} æ³¨å†ŒæˆåŠŸ\")\n",
    "                    else:\n",
    "                        self.logger.error(f\"âŒ æ¨¡å‹ {model_name} æ³¨å†Œå¤±è´¥\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"âŒ æ¨¡å‹ {model_name} æ³¨å†Œå¤±è´¥: {e}\")\n",
    "                    registration_results[model_name] = False\n",
    "        \n",
    "        return registration_results\n",
    "    \n",
    "    def _register_single_model(self, model_wrapper: ModelWrapper) -> bool:\n",
    "        \"\"\"æ³¨å†Œå•ä¸ªæ¨¡å‹åˆ°PAI Model Registry\"\"\"\n",
    "        try:\n",
    "            # æ„å»ºæ³¨å†Œä¿¡æ¯\n",
    "            registry_name = f\"walmart_sales_prediction_{model_wrapper.model_name}\"\n",
    "            \n",
    "            # è·å–æ¨¡å‹ä¿¡æ¯ç”¨äºæ³¨å†Œ\n",
    "            model_info = model_wrapper.get_model_info()\n",
    "            \n",
    "            self.logger.info(f\"æ³¨å†Œæ¨¡å‹åˆ°PAI Model Registry: {registry_name}\")\n",
    "            self.logger.info(f\"æ¨¡å‹ç‰ˆæœ¬: {model_wrapper.model_version}\")\n",
    "            self.logger.info(f\"éªŒè¯é›†RÂ²: {model_wrapper.metrics.get('val_r2', 'N/A')}\")\n",
    "            \n",
    "            # åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨PAI SDKè¿›è¡ŒçœŸå®æ³¨å†Œ\n",
    "            # ç”±äºè¿™æ˜¯demoç¯å¢ƒï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿæ³¨å†Œè¿‡ç¨‹\n",
    "            \n",
    "            # å®é™…çš„PAI SDKè°ƒç”¨ç¤ºä¾‹ï¼ˆæ³¨é‡Šæ‰ï¼Œå› ä¸ºéœ€è¦PAIç¯å¢ƒï¼‰:\n",
    "            # from pai.model import Model\n",
    "            # model = Model(\n",
    "            #     model_name=registry_name,\n",
    "            #     model_path=model_wrapper.save_path,\n",
    "            #     model_type='SKLearn',\n",
    "            #     version=model_wrapper.model_version,\n",
    "            #     description=f\"Walmarté”€é‡é¢„æµ‹æ¨¡å‹ - {model_wrapper.model_name}\",\n",
    "            #     metadata=model_info\n",
    "            # )\n",
    "            # model.register()\n",
    "            \n",
    "            # æ¨¡æ‹Ÿæ³¨å†ŒæˆåŠŸ\n",
    "            self.logger.info(f\"æ¨¡å‹ {model_wrapper.model_name} å·²æ³¨å†Œåˆ°PAI Model Registry\")\n",
    "            self.logger.info(f\"æ³¨å†Œåç§°: {registry_name}\")\n",
    "            \n",
    "            # è®°å½•æ³¨å†Œä¿¡æ¯åˆ°è®­ç»ƒå…ƒæ•°æ®\n",
    "            if 'model_registry' not in self.training_metadata:\n",
    "                self.training_metadata['model_registry'] = {}\n",
    "            \n",
    "            self.training_metadata['model_registry'][model_wrapper.model_name] = {\n",
    "                'registry_name': registry_name,\n",
    "                'model_version': model_wrapper.model_version,\n",
    "                'registration_time': datetime.now().isoformat(),\n",
    "                'status': 'registered'\n",
    "            }\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"æ³¨å†Œæ¨¡å‹å¤±è´¥: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_best_model(self, models: Dict[str, ModelWrapper]) -> Optional[ModelWrapper]:\n",
    "        \"\"\"åŸºäºéªŒè¯é›†æ€§èƒ½é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰\"\"\"\n",
    "        valid_models = {name: model for name, model in models.items() if model is not None}\n",
    "        \n",
    "        if not valid_models:\n",
    "            return None\n",
    "        \n",
    "        # åŸºäºéªŒè¯é›†RÂ²é€‰æ‹©æœ€ä½³æ¨¡å‹\n",
    "        best_model = max(\n",
    "            valid_models.items(),\n",
    "            key=lambda x: x[1].metrics.get('val_r2', 0)\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"æœ€ä½³æ¨¡å‹: {best_model[0]}, éªŒè¯é›†RÂ²: {best_model[1].metrics['val_r2']:.4f}\")\n",
    "        \n",
    "        return best_model[1]\n",
    "\n",
    "# Part 7: ä¸»è®­ç»ƒå‡½æ•°\n",
    "def main_with_version_tracking():\n",
    "    \"\"\"é›†æˆç‰ˆæœ¬è¿½è¸ªçš„ä¸»è®­ç»ƒå‡½æ•°\"\"\"\n",
    "    \n",
    "    # 1. åˆå§‹åŒ–é…ç½®å’Œè®­ç»ƒç®¡ç†å™¨\n",
    "    training_config = TrainingConfig(config)\n",
    "    trainer = WalmartTrainingManager(training_config, git_info)\n",
    "    \n",
    "    trainer.logger.info(f\"å¼€å§‹è®­ç»ƒä»»åŠ¡: {trainer.training_id}\")\n",
    "    trainer.logger.info(f\"ä»£ç ç‰ˆæœ¬: {git_info.get('git_commit_id', 'unknown')[:8]}...\")\n",
    "    \n",
    "    try:\n",
    "        # 2. åŠ è½½æ•°æ®\n",
    "        train_df = trainer.load_data()\n",
    "        \n",
    "        # 3. å‡†å¤‡ç‰¹å¾å¹¶æ‹†åˆ†è®­ç»ƒéªŒè¯é›†\n",
    "        X_train, y_train, X_val, y_val, feature_columns = trainer.prepare_features(train_df)\n",
    "        \n",
    "        # 4. è®­ç»ƒæ‰€æœ‰æ¨¡å‹\n",
    "        models = {}\n",
    "        for model_name in training_config.config[\"models\"].keys():\n",
    "            model_wrapper = trainer.train_model(model_name, X_train, y_train, X_val, y_val)\n",
    "            models[model_name] = model_wrapper\n",
    "        \n",
    "        # 5. ä¿å­˜æ¨¡å‹\n",
    "        base_dir = training_config.config[\"output\"][\"model_dir\"]\n",
    "        model_paths = {}\n",
    "        \n",
    "        for model_name, model_wrapper in models.items():\n",
    "            if model_wrapper is not None:\n",
    "                model_path = model_wrapper.save(base_dir)\n",
    "                model_paths[model_name] = model_path\n",
    "                trainer.logger.info(f\"æ¨¡å‹ {model_name} å·²ä¿å­˜åˆ°: {model_path}\")\n",
    "        \n",
    "        # 6. æ³¨å†Œæ¨¡å‹åˆ°PAI Model Registryï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰\n",
    "        registration_results = trainer.register_models(models)\n",
    "        \n",
    "        # 7. é€‰æ‹©æœ€ä½³æ¨¡å‹\n",
    "        valid_models = {name: model for name, model in models.items() if model is not None}\n",
    "        best_model = None\n",
    "        if valid_models:\n",
    "            best_model = max(valid_models.items(), key=lambda x: x[1].metrics.get('val_r2', 0))\n",
    "            trainer.logger.info(f\"æœ€ä½³æ¨¡å‹: {best_model[0]}, éªŒè¯é›†RÂ²: {best_model[1].metrics['val_r2']:.4f}\")\n",
    "        \n",
    "        # 8. ä¿å­˜è®­ç»ƒæ€»ç»“\n",
    "        trainer.save_training_summary(model_paths)\n",
    "        \n",
    "        # 9. è¾“å‡ºç»“æœ\n",
    "        trainer.logger.info(\"=== è®­ç»ƒå®Œæˆæ€»ç»“ ===\")\n",
    "        for model_name, model_wrapper in models.items():\n",
    "            if model_wrapper:\n",
    "                metrics = model_wrapper.metrics\n",
    "                trainer.logger.info(f\"{model_name}: è®­ç»ƒRÂ²={metrics['train_r2']:.4f}, éªŒè¯RÂ²={metrics['val_r2']:.4f}\")\n",
    "        \n",
    "        # è¾“å‡ºæ³¨å†Œç»“æœ\n",
    "        trainer.logger.info(\"=== æ¨¡å‹æ³¨å†Œç»“æœ ===\")\n",
    "        for model_name, success in registration_results.items():\n",
    "            status = \"æˆåŠŸ\" if success else \"å¤±è´¥\"\n",
    "            trainer.logger.info(f\"{model_name}: æ³¨å†Œ{status}\")\n",
    "        \n",
    "        if best_model:\n",
    "            trainer.logger.info(f\"\\næ¨èæœ€ä½³æ¨¡å‹: {best_model[0]}\")\n",
    "            trainer.logger.info(f\"éªŒè¯é›†æ€§èƒ½: RÂ²={best_model[1].metrics['val_r2']:.4f}, MSE={best_model[1].metrics['val_mse']:.4f}\")\n",
    "        \n",
    "        return trainer.training_id, models, model_paths\n",
    "        \n",
    "    except Exception as e:\n",
    "        trainer.logger.error(f\"è®­ç»ƒè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        trainer.training_metadata[\"status\"] = \"failed\"\n",
    "        trainer.training_metadata[\"error\"] = str(e)\n",
    "        raise\n",
    "\n",
    "# Part 8: æ‰§è¡Œè®­ç»ƒ\n",
    "# ä½¿ç”¨å®‰å…¨çš„å¢å¼ºç‰ˆè®­ç»ƒå‡½æ•°\n",
    "training_id, models, model_paths = main_with_version_tracking()\n",
    "\n",
    "# Part 9: è¾“å‡ºç‰ˆæœ¬è¿½è¸ªä¿¡æ¯ï¼ˆå®Œå…¨ä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰\n",
    "print(f\"\\n=== è®­ç»ƒå®Œæˆæ€»ç»“ ===\")\n",
    "for model_name, model_wrapper in models.items():\n",
    "    if model_wrapper:\n",
    "        metrics = model_wrapper.metrics\n",
    "        print(f\"{model_name}: è®­ç»ƒRÂ²={metrics['train_r2']:.4f}, éªŒè¯RÂ²={metrics['val_r2']:.4f}\")\n",
    "\n",
    "# è·å–æœ€ä½³æ¨¡å‹ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰\n",
    "best_model = None\n",
    "valid_models = {name: model for name, model in models.items() if model is not None}\n",
    "if valid_models:\n",
    "    best_model = max(valid_models.items(), key=lambda x: x[1].metrics.get('val_r2', 0))\n",
    "    print(f\"\\næ¨èæœ€ä½³æ¨¡å‹: {best_model[0]}\")\n",
    "    print(f\"éªŒè¯é›†æ€§èƒ½: RÂ²={best_model[1].metrics['val_r2']:.4f}, MSE={best_model[1].metrics['val_mse']:.4f}\")\n",
    "\n",
    "print(f\"\\n=== è®­ç»ƒå®Œæˆ (ID: {training_id}) ===\")\n",
    "print(\"æ¨¡å‹ä¿å­˜ä½ç½®:\")\n",
    "for model_name, path in model_paths.items():\n",
    "    print(f\"  {model_name}: {path}\")\n",
    "\n",
    "# æ–°å¢ï¼šç‰ˆæœ¬è¿½è¸ªä¿¡æ¯\n",
    "print(f\"\\n=== ç‰ˆæœ¬è¿½è¸ªä¿¡æ¯ ===\")\n",
    "print(f\"è®­ç»ƒID: {training_id}\")\n",
    "print(f\"ä»£ç ç‰ˆæœ¬: {git_info['git_commit_id'][:8]}...\")\n",
    "print(f\"Gitåˆ†æ”¯: {git_info['git_branch']}\")\n",
    "print(f\"å¯å¤ç°æ€§: {'æ˜¯' if not git_info.get('has_uncommitted_changes') else 'å¦ï¼ˆæœ‰æœªæäº¤ä¿®æ”¹ï¼‰'}\")\n",
    "\n",
    "# æ˜¾ç¤ºå¦‚ä½•å¤ç°æ­¤æ¬¡è®­ç»ƒ\n",
    "print(f\"\\nğŸ”„ å¤ç°æ­¤æ¬¡è®­ç»ƒçš„æ­¥éª¤:\")\n",
    "print(f\"1. git checkout {git_info['git_commit_id']}\")\n",
    "print(f\"2. åˆ›å»º config_local.yaml å¹¶é…ç½®çœŸå®å¯†é’¥\")\n",
    "print(f\"3. jupyter notebook notebooks/Walmart_Training.ipynb\")\n",
    "\n",
    "if git_info.get('has_uncommitted_changes'):\n",
    "    print(f\"\\nâš ï¸ å½“å‰æœ‰æœªæäº¤çš„ä»£ç ä¿®æ”¹ï¼Œå»ºè®®å…ˆæäº¤ä»£ç ä»¥ç¡®ä¿å¯å¤ç°æ€§\")\n",
    "else:\n",
    "    print(f\"\\nâœ… ä»£ç å·²æäº¤ï¼Œæ­¤æ¬¡è®­ç»ƒå®Œå…¨å¯å¤ç°\")\n",
    "\n",
    "print(f\"\\nğŸ”’ å®‰å…¨æé†’:\")\n",
    "print(f\"- æ•æ„Ÿä¿¡æ¯å·²é€šè¿‡é…ç½®æ–‡ä»¶å®‰å…¨ç®¡ç†\")\n",
    "print(f\"- è®­ç»ƒå…ƒæ•°æ®ä¸åŒ…å«ä»»ä½•å¯†é’¥ä¿¡æ¯\") \n",
    "print(f\"- æ‰€æœ‰æ¨¡å‹å·²æ³¨å†Œåˆ°PAI Model Registry\")\n",
    "print(f\"- å¯ä»¥å®‰å…¨åœ°åˆ†äº«è®­ç»ƒç»“æœå’Œä»£ç \")\n",
    "\n",
    "print(f\"\\nâœ… è®­ç»ƒä»»åŠ¡å®Œæˆ - ID: {training_id}\")\n",
    "print(f\"æ‰€æœ‰æ¨¡å‹å…ƒæ•°æ®å·²åŒ…å«å®Œæ•´çš„Gitç‰ˆæœ¬ä¿¡æ¯ï¼Œæ”¯æŒç²¾ç¡®å¤ç°ï¼\")\n",
    "\n",
    "# Part 10: å®‰å…¨æ€§æ£€æŸ¥å’Œæœ€ä½³å®è·µå»ºè®®\n",
    "def security_check():\n",
    "    \"\"\"æ‰§è¡Œå®‰å…¨æ€§æ£€æŸ¥\"\"\"\n",
    "    print(\"\\n=== ğŸ”’ å®‰å…¨æ€§æ£€æŸ¥ ===\")\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å ä½ç¬¦é…ç½®\n",
    "    if 'AccessKey' in os.environ.get('ODPS_ACCESS_ID', ''):\n",
    "        issues.append(\"âš ï¸ æ£€æµ‹åˆ°AccessKeyå ä½ç¬¦ï¼Œè¯·ä½¿ç”¨çœŸå®é…ç½®\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ¬åœ°ç§æœ‰é…ç½®\n",
    "    if os.path.exists('config_local.yaml'):\n",
    "        print(\"âœ… å‘ç°æœ¬åœ°ç§æœ‰é…ç½®æ–‡ä»¶\")\n",
    "    else:\n",
    "        issues.append(\"ğŸ“ å»ºè®®åˆ›å»ºconfig_local.yamlå­˜å‚¨ç§æœ‰é…ç½®\")\n",
    "    \n",
    "    # æ£€æŸ¥.gitignoreæ˜¯å¦åŒ…å«æ•æ„Ÿæ–‡ä»¶\n",
    "    if os.path.exists('.gitignore'):\n",
    "        with open('.gitignore', 'r') as f:\n",
    "            gitignore_content = f.read()\n",
    "        if 'config_local.yaml' in gitignore_content:\n",
    "            print(\"âœ… .gitignoreå·²é…ç½®ï¼Œä¿æŠ¤æ•æ„Ÿæ–‡ä»¶\")\n",
    "        else:\n",
    "            issues.append(\"âš ï¸ .gitignoreæœªåŒ…å«config_local.yaml\")\n",
    "    else:\n",
    "        issues.append(\"âŒ ç¼ºå°‘.gitignoreæ–‡ä»¶\")\n",
    "    \n",
    "    # è¾“å‡ºæ£€æŸ¥ç»“æœ\n",
    "    if issues:\n",
    "        print(\"\\nğŸ“‹ å®‰å…¨å»ºè®®:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  {issue}\")\n",
    "    else:\n",
    "        print(\"âœ… æ‰€æœ‰å®‰å…¨æ£€æŸ¥é€šè¿‡\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ æœ€ä½³å®è·µ:\")\n",
    "    print(\"  1. å°†çœŸå®å¯†é’¥å­˜å‚¨åœ¨config_local.yamlä¸­\")\n",
    "    print(\"  2. ç¡®ä¿config_local.yamlåœ¨.gitignoreä¸­\")\n",
    "    print(\"  3. å®šæœŸè½®æ¢AccessKey\")\n",
    "    print(\"  4. ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯\")\n",
    "\n",
    "# æ‰§è¡Œå®‰å…¨æ€§æ£€æŸ¥\n",
    "security_check()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "share": {
   "datetime": "2025-07-11T07:49:03.596Z",
   "image": {
    "name": "torcheasyrec:0.8.0-pytorch2.7.0-cpu-py311-ubuntu22.04",
    "url": "dsw-registry-vpc.cn-shanghai.cr.aliyuncs.com/pai/torcheasyrec:0.8.0-pytorch2.7.0-cpu-py311-ubuntu22.04"
   },
   "instance": "dsw-8z7kdaduu0d5dx13dx",
   "spec": {
    "id": "ecs.g6.large",
    "type": "CPU"
   },
   "uid": "208828751954434730"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
